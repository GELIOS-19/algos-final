% --------- document setup ---------
\documentclass[12pt]{article}
\usepackage[
    left=1in,
    right=1in,
    top=0.5in,
    bottom=0.5in,
    includehead,
    includefoot
]{geometry}
\setlength{\parskip}{0.75em} % larger jumps at end of paragraph
\setlength{\parindent}{0pt} % no indent
\pagenumbering{gobble} % no page numbers

% --------- packages ---------
\usepackage{graphicx} % required for inserting images
\usepackage{multicol} % two column feature
\usepackage{hyperref} % hyperlinks
\usepackage{etoolbox} % if-then-else logic
\usepackage{amsmath, amsfonts, amssymb, amsthm, mathrsfs} % math
\usepackage{float} % figure env
\usepackage{tikz} % drawing things
\usetikzlibrary{automata, positioning}
\usepackage{xcolor} % colors
\usepackage[most]{tcolorbox} % answer boxes
\usepackage{fancyhdr} % header
\usepackage{fancyvrb} % fancy verbatim
\usepackage{ifthen} % conditional statements
\usepackage{lipsum} % filler text
\usepackage{xstring} % For string manipulation
\usepackage{enumitem} % for hint hanging indent
\usepackage{outlines} % for outlines
\usepackage{fancybox} % for nice title page box
\usepackage{soul} % for strikethrough
\usepackage{algorithm} % for algorithms
\usepackage{dsfont}

\newcommand{\note}[1]{{\footnotesize
        \begin{description}
                [leftmargin=3.4em,style=nextline]
            \item[Note:] {#1}
        \end{description}}
}

% --------- Header + Title ---------
\renewcommand\maketitle{
    \begin{center}
        \shadowbox{\parbox{5.3in}{{\small {\bf {\sc Georgia Tech Spring 2025
                \hfill CS 3511: Honors Algorithms}}}
                \medbreak
                \begin{center}
                    {\large \bfseries \reportTitle}
                \end{center}
                \medbreak
                Authors: {\em \authors}
                \newline
                Date: \reportDate
            \vspace{2pt}}
        }
        \vspace{6pt}
    \end{center}
}

% --------- Custom symbols ---------
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\Ptime}{\mathrm{P}}
\newcommand{\NPtime}{\mathrm{NP}}
\newcommand{\poly}{\text{poly}}
\newcommand{\Alg}{\mathcal{A}}
\newcommand{\Opt}{\operatorname{OPT}}
\newcommand{\dx}{\ensuremath{\mathrm{d}x}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor} % for floor
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil} % for ceiling

% ------------ Document ------------
\begin{document}

\newcommand{\reportTitle}{Final Project}
\newcommand{\reportDate}{Wednesday 4/16/25}

\newcommand{\authors}{Arjun Sheshadri, Vilohith Gokarakonda, Yiqi Sun, Tyler Huang}

\maketitle

\section{Introduction}

Suppose if you have a graph $G = (V, E)$ with a given adjacency matrix, which represents where any two nodes are connected to each other. Suppose you wanted to find the shortest path to one node, using all nodes as a starting point.

Shortest distances between all pairs of nodes in a graph is important for its real world applications in communication networks, social graphs, and more. Edges in such graphs can be added and removed at any time, so being able to efficiently maintain all pairs distances is crucial. Using the Floyd-Warshall Algorithm, we are able to accomplish this in $\bigO(n^3)$ time. However, this becomes inefficient for larger or frequently changing graphs.

This report explores a \textit{dynamic algorithm} described in Jan van den Brand's \href{https://www.dropbox.com/scl/fi/zjfbk8dbxbzcn05dr8l11/fall22_daa_lecturenotes.pdf?rlkey=g6z7z1hvj7jknvlpy1s3dqcjw&e=1&st=13r18doq&dl=0}{notes} for maintaining All-Pairs-Shortest-Paths (\texttt{APSP}) in directed graphs with or without edge weights. Instead of recalculating shortest distances from scratch every update, it efficiently maintains a data structure where all distances can be updated in $\displaystyle \tilde \bigO(n^{2.5})$ time when an edge to a single vertex is added or removed. Specifically, it uses concepts from dynamic algebraic algorithms, involving polynomial matrix inverses to represent path information, and extending them to full distances with random sampling and Dijkstra's algorithm.

In this report, we present the problem statement, and the technical background relevant to the paper, including ring algebra, polynomial matrices, and how edges are updated. We also cover the steps to the solution, and connect this approach to the original groundbreaking \href{https://www.dropbox.com/scl/fi/5w4k0lg0u8e36zugapp6c/Sankowski-COCOON-05-subquadratic-dynamic-distances.pdf?rlkey=5eahwsl7a49kwxyvlr7yqyoz3&e=2&st=d3zo7owc&dl=0}{paper} by Sankowski, which can do updates in $\displaystyle \tilde \bigO(n^{1.932})$  randomized time and queries in $\displaystyle \tilde \bigO(n^{1.288})$ randomized time. Specifically, we compare their mathematical foundations, update operations, applications, and time complexities.

\section{Problem Statement}

Given a graph $G(V, E)$, we want to develop a data structure that can maintain \texttt{APSP} dynamically with an initial overhead $\displaystyle \tilde \bigO(n^{3.5})$ time and supports queries and updates in $\displaystyle \tilde \bigO(n^{2.5})$ time. Since this algorithm is optimized for maintaining \texttt{APSP} for changing graphs (hence requires a dynamic algorithm), having a time complexity of $\displaystyle \tilde \bigO(n^{2.5})$ is more efficient compared to the naive approach which takes $\bigO(n^3)$ time for queries and updates.

\newpage

Specifically, we will be working towards proving the following theorem:

\textbf{Theorem 1.0.0}: \emph{There exists a data structure that supports the following operations}:
\begin{enumerate}
    \item \textsc{Initialize($G = (V, E)$)} \emph{Initialize an $n$-node graph and return \texttt{APSP} in $\displaystyle \tilde \bigO(n^{3.5})$ time.}
    \item \textsc{Update($v,\,E^+,\,E^-$)} \emph{Given a vertex $v$ and two sets of edges $E^+\subseteq(\{v\}\times V\cup V\times\{v\})$ to insert and $E^-$ to delete (all incident to $v$), update $G$ and return the new \texttt{APSP} in $\displaystyle \tilde \bigO(n^{2.5})$ time.}
\end{enumerate}

Additionally, we will be extending this theorem using Sankowski's paper:

\textbf{Theorem 1.0.1}: \emph{There exists a data structure that supports the following operations}:
\begin{enumerate}
    \item \textsc{Initialize($G = (V, E)$)} \emph{Preprocess in $\displaystyle \tilde \bigO(n^{3})$ time.}
    \item \textsc{Update($e$)} \emph{Insert or delete a single edge $e\in V\times V$ in $\bigO(n^{1.932})$ randomized time.}
    \item \textsc{Query($s, t$)} \emph{Return the current distance $\mathrm{dist}_G(s,t)$ in $\bigO(n^{1.288})$ randomized time.}
\end{enumerate}
\emph{All operations succeed with high probability over the random choices.}

We will be covering general overviews of topics that help toward understanding the intuition behind these theorems in the next section.

\section{Technical Background} \label{sec:3}

\subsection{Matrix Multiplication Recap and Naive APSP} \label{sec:3.1}

Matrix multiplication is an important concept that we will be making extensive use of to understand the technical components in this report. You are already familiar with the basic method of multiplying two matrices with the same inner dimension, where you multiply the $n$-th row of the first matrix with the $n$-th column of the second matrix. Below is a visual representation:

\[
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
    \begin{bmatrix}
        e & f \\
        g & h
    \end{bmatrix}
    =
    \begin{bmatrix}
        a \cdot e + b \cdot g & a \cdot f + b \cdot h \\
        c \cdot e + d \cdot g & c \cdot f + d \cdot h
    \end{bmatrix}
\]

Notice how in each $(i, j)$ entry of the product matrix, we compute the \emph{dot product} of the vector that forms the $i$-th row of the first matrix and the $j$-th column of the second matrix. The \emph{dot product} is simply an operation on the entries of two vectors involving the typical $+$ and $\times$ operations.

Let's take a quick detour from matrix multiplication to talk about the well-known Floyd-Warshall algorithm for computing \texttt{APSP}. The key realization to understanding this algorithm is that between some start node $s$ and some terminal node $t$, the shortest path from $s \to t$ is the shorter path we can get from directly traveling from $s \to t$ vs. the path we get by traveling from $s$ to some intermediate node $k$ and then from $k$ to $t$. For a visualization, consider the trivial example below:

\begin{center}
    \includegraphics[scale=0.5]{media/Figure_1.png}
\end{center}

In the above graphic, the direct path from $s \to t$ has a cost of 11, while the path from $s \to k$ has a cost of 5 and the path from $k \to t$ has a cost of 5 as well. We can see that if we take $\min \{\mathrm{dist}_{s,t} = 11, \mathrm{dist}_{s,k} + \mathrm{dist}_{k,t} = 10 \}$, we get that by considering the intermediate node $k$, we arrive at a cheaper path from $s$ to $t$.

How does the above logic tie into matrix multiplication? Well, it might be intuitive to think of graphs in terms of adjacency matrices and the core Floyd-Warshall computation as a dot product between some vector $u$ that represents the shortest known paths from the starting node $s$ to all possible intermediate nodes $k$, and another vector $v$ that represents the shortest paths from all possible intermediate nodes $k$ to the terminal node $t$. Let's see an example of this analogy on the simple graph presented earlier:

\begin{quote}
    Consider the adjacency matrix $A$ for the graph presented earlier:

    \[
        A =
        \bordermatrix{
            & s & k & t \cr
            s & 0 & 5 & 11 \cr
            k & \infty & 0 & 5 \cr
            t & \infty & \infty & 0 \cr
        }
    \]

    Notice that in $A$, the $(i, j)$ entry represents the cost of the \emph{direct} path from node $i$ to node $j$. The rows and columns are labeled with their appropriate nodes on the borders.

    For the sake of computation, let's take the row $s$ and the column $t$ of the matrix as our $u$ and $v$ vectors. That is, let

    \[
        u =
        \begin{bmatrix}
            0 & 5 & 11
        \end{bmatrix}
        \qquad
        v =
        \begin{bmatrix}
            11 \\
            5 \\
            0
        \end{bmatrix}
    \]

    Just for clarity, recall that $u$ represents the shortest known path between the starting node $s$ and every other node $k$ in the graph. For column $s$ of $u$, the shortest path cost is 0 since we are already at $s$. For column $k$ of $u$, the path cost is 5 since the edge $s \to k$ on the graph has a cost of 5. For column $t$ of $u$, the path cost is 11 since the edge $s \to t$ has a cost of 11. In general, we see that each \emph{column} in $u$ represents the cost of the edge $s \to \textit{column in u}$. Following a similar logic, notice that in the vector $v$, each \emph{row} represents the cost of the edge $\textit{row in v} \to t$. Both \textit{column in $u$} and \textit{row in $v$} represent all the nodes in the graph.

    Let's replace the numbers in $A$ with symbols, and see what happens when we take the dot product of $u$ and $v$:

    \[
        A =
        \bordermatrix{
            & s & k & t \cr
            s & \mathrm{dist}_{s \to s} & \mathrm{dist}_{s \to k} & \mathrm{dist}_{s \to t} \cr
            k & \mathrm{dist}_{k \to s} & \mathrm{dist}_{k \to k} & \mathrm{dist}_{k \to t} \cr
            t & \mathrm{dist}_{t \to s} & \mathrm{dist}_{t \to k} & \mathrm{dist}_{t \to t} \cr
        }
    \]

    \[
        u =
        \begin{bmatrix}
            \mathrm{dist}_{s \to s} & \mathrm{dist}_{s \to k} & \mathrm{dist}_{s \to t}
        \end{bmatrix}
        \quad
        v =
        \begin{bmatrix}
            \mathrm{dist}_{s \to t} \\
            \mathrm{dist}_{k \to t} \\
            \mathrm{dist}_{t \to t}
        \end{bmatrix}
    \]

    Let's see what happens when we take the dot product of $u$ and $v$, which will be the $(s, t)$ entry of the matrix $A^2$:

    \begin{eqnarray}
        A^2_{(s,t)}
        &= \mathrm{dist}_{s \to s} \times \mathrm{dist}_{s \to t} \\
        &+ \; \mathrm{dist}_{s \to k} \times \mathrm{dist}_{k \to t} \\
        &+ \; \mathrm{dist}_{s \to t} \times \mathrm{dist}_{t \to t}
    \end{eqnarray}

    Based on the dot product formula, it seems that in the $(s, t)$ entry of the matrix $A^2$, we are able to calculate some information involving all paths from node $s$ to node $t$ involving one intermediate step through all the other nodes in the graph. Particularly, the product on line $(1)$ gives us information regarding the path $s \to s \to t$, the product on line $(2)$ gives us information regarding the path $s \to k \to t$, and the product on line $(3)$ gives us information regarding the path $s \to t \to t$. You may notice the paths $s \to s \to t$ and $s \to t \to t$ are simply the same as the path $s \to t$, a direct path from $s$ to $t$ with no intermediate nodes. It may become apparent to you that by induction, the matrix $A^3_{(s, t)}$ stores the shortest path between $s \to t$, $s \to k \to t$ for all graph vertices $k$, and $s \to k_0 \to k_1 \to t$ for all combinations of nodes $k_0$ and $k_1$ in the graph. We can say that the adjacency matrix $A^n_{(s, t)}$ stores the shortest path from $s$ to $t$ using \emph{up to} $n$ intermediate vertices.

    {\footnotesize Why is the induction apparent? If we multiply adjacency matrix $A^n$ by adjacency matrix $A^m$, we notice that we will be considering paths of the form $s \to k_0 \to \cdots \to k_{n - 1} \to k_{n} \to k_{n + 1} \to \cdots k_{n + m - 1} \to t$. The total length of this path is $n + m$, which is also the power of the product as $A^n \times A^m = A^{n + m}$, and the number of intermediate nodes is $n + m - 1$.}
\end{quote}

As shown in the above analogy, we are essentially able to store the result of a dot product operation between the path lengths of $s \to t$ and $s \to k \to t$ for every intermediate node $k$ in the graph. As we explained earlier during our discussion of Floyd-Warshall's algorithm, we specifically need to compute the minimum cost path between all the aforementioned products from the dot product calculation. In the next section, we will show how we can override the standard $+$ and $\times$ operations such that simply computing the squares of adjacency matrices allows us to solve \texttt{APSP}, and we will also explain how other properties of environments with overridden $+$ and $\times$ operations may help us solve \texttt{APSP} more efficiently.

\subsection{Abstract Algebra Overview and Ring Operations} \label{sec:3.2}

In this section, we cover the basics of the field of abstract algebra as relevant in solving the \texttt{APSP} problem. As we showed in section~\ref{sec:3.1}, on an adjacency matrix of power $A^n$, the $(s, t)$ entry encodes information regarding the dot product between $s$-$t$ paths involving up to $n$ intermediate nodes. When we explained our theory regarding Floyd-Warshall's algorithm, we mentioned that we somehow need to store the minimum cost path between all of the path costs ($s \to t, s \to k_0 \to t, \ldots, s \to k_0 \to k_1 \to \cdots \to k_{n - 1} \to t$) that make up the individual products in the dot product. We may now ask ourselves, when computing a dot product in the form:
\[
    \begin{split}
        A^2_{(s,t)}
        &= \mathrm{dist}_{s \to s} \times \mathrm{dist}_{s \to t} \\
        &+ \; \mathrm{dist}_{s \to k} \times \mathrm{dist}_{k \to t} \\
        &+ \; \mathrm{dist}_{s \to t} \times \mathrm{dist}_{t \to t}
    \end{split}
\]
Is it possible to override the $+$ and $\times$ operations in order to suit our desired $\min$ and $+$ operations?

It turns out this is entirely possible as long as our overridden $+$ and $\times$ operations adhere to the rules of \emph{algebraic rings}. A ring is an algebraic structure—that is, a set of elements together with two binary operations $\oplus$ and $\otimes$—that satisfies certain axioms:

\begin{quote}
    $S$ is an algebraic \emph{semiring} defined as $(S, \oplus, \otimes)$. The below conditions must be met:

    \begin{enumerate}
        \item $(S, \oplus)$ is an abelian group. In other words, it satisfies the commutative property that $\forall a, b \in S : a \oplus b = b \oplus a$ and it satisfies the identity that $\forall a \in S : a \oplus 0_S = a$, where $0_S$ refers to the zero element in $S$.
        \item $(S, \otimes)$ is a monoid. In other words, it satisfies the identity $\forall a \in S : a \otimes I = a$, where $I$ refers to the identity element, which can typically be thought of as 1.
        \item The $\otimes$ operation must distribute over the $\oplus$ operation. In other words, $\forall a, b, c \in S : a \otimes (b \oplus c) = a \otimes b \oplus a \otimes c$.
        \item $0_S$ is an annihilator in $\otimes$. In other words, $\forall a \in S : a \otimes 0_S = 0_S \otimes a = 0_S$.
    \end{enumerate}

    Alternatively, we define $F$ as an algebraic \emph{field}, denoted as $(F, \oplus, \otimes)$ if it meets the below conditions:

    \begin{enumerate}
        \item $(F, \oplus)$ is an abelian group.
        \item $(F \backslash \{0\}, \otimes)$ is also an abelian group, which implies the existence of a multiplicative inverse for every non-zero element.
    \end{enumerate}

    {\footnotesize Note, the difference between a \emph{ring} and a \emph{semiring} is that a semiring does not need to support the additive inverse. Meaning, there does not necessarily have to be a ``negative'' corresponding to each element. Additionally, note that $\otimes$ does not have to be commutative.}
\end{quote}

So how does this tie into our \texttt{APSP} problem?

We can define our dot product over the semiring $(S, \oplus, \otimes)$ where:
\[
    \begin{split}
        & a, b \in \mathds{R} \\
        & a \oplus b = \min\{a, b\} \\
        & a \otimes b = a + b \qquad \text{{\footnotesize Where + is the typical addition}}
    \end{split}
\]
It is straightforward why we can do this, since $(S, \min, +)$ satisfies all of the axioms. You can convince yourself of this fact. It is important to note that $S$ does not have an additive inverse since there is no possible $a, b \in S : \min\{a, b\} = \infty$ unless $a = \infty$ and $b = \infty$.

Now, notice how the dot product defined at the end of section~\ref{sec:3.1} changes. Under our ring $S$, we get:
\[
    \begin{split}
        A^2_{(s,t)}
        &= \mathrm{dist}_{s \to s} \otimes \mathrm{dist}_{s \to t} \\
        &\oplus \; \mathrm{dist}_{s \to k} \otimes \mathrm{dist}_{k \to t} \\
        &\oplus \; \mathrm{dist}_{s \to t} \otimes \mathrm{dist}_{t \to t}
    \end{split}
    \qquad = \qquad
    \min\!\left\{
        \begin{aligned}
            &\mathrm{dist}_{s\to s} + \mathrm{dist}_{s\to t},\\
            &\mathrm{dist}_{s\to k} + \mathrm{dist}_{k\to t},\\
            &\mathrm{dist}_{s\to t} + \mathrm{dist}_{t\to t}
        \end{aligned}
    \right\}
\]
This exactly what we wanted to compute when we discussed Floyd Warshall's algorithm! As you can see, using algebraic rings, we have come up with a system where simple repeated squaring of the adjacency matrix $A$ over the ring $R$ effectively solves \texttt{APSP}. We notice that after squaring the adjacency matrix $n - 1$ times, the result contains the answer to \texttt{APSP}. Since matrix multiplication takes on the order of $\bigO(v^3)$ time, where $v$ is the number of vertices in the graph, and since we must effectively square the adjacency matrix $v$ times, we can see how a min-plus squaring algorithm attains a time complexity of $\bigO(v^4)$ (using naive matrix multiplication).

Our repeated squaring approach can take as little as $\bigO(n^3 \log(n))$ time if we are more efficient in how we implement \emph{binary exponentiation} rather than using naive matrix multiplication for exponentiation. However, the repeated squaring method is still more inefficient than Floyd-Warshall's algorithm, which takes only $\bigO(n^3)$ time. It turns out that both the min-plus repeated squaring approach and Floyd-Warshall's algorithm are performing the same operations to compute \texttt{APSP}, however Floyd-Warshall's algorithm attains more efficiency using \emph{dynamic programming} and ``simulating'' the min-plus matrix multiplications in place. This min-plus repeated squaring approach also lies at the heart of the algorithm mentioned in Jan van den Brand's paper, but before we can conceptualize it, we are going to use our knowledge or algebraic rings to talk about polynomial matrices.

\subsection{Polynomial Matrices} \label{sec:3.3}

Now, let's really start building the foundation to Jan van den Brand's \texttt{APSP} algorithm by taking about adjacency matrices with polynomial entries!

\subsubsection{Unweighted Adjacency Matrices}

First let's build some intuition for why we might want to build polynomial adjacency matrices. Going forward, we are specifically going to talk about \emph{unweighted} adjacency matrices where $A_{(s, t)} = 1$ if there is a directed edge $s \to t$ and $0$ otherwise. When we refer to simple unweighted adjacency matrices, it turns out we can use simple powers of $A$ (ie. $A, A^2, A^3, \ldots, A^n$) and the traditional operations $+$ and $\times$ to achieve a similar effect to our min-plus repeated squaring algorithm that we explained in section~\ref{sec:3.2}. However, instead of tracking in the $A^n_{(s, t)}$ the shortest path between start node $s$ and terminal node $t$ using up to $n$ intermediate nodes, in the $A^n_{(s, t)}$ position, we are instead tracking the \emph{number} of paths that \emph{exist} between $s$ and $t$ on \emph{exactly} $n$ nodes. Why exactly is this? Let's consider a simple example with the graph below:

\begin{center}
    \includegraphics[scale=0.5]{media/Figure_2.png}
\end{center}

\begin{quote}
    We can build the unweighted adjacency matrix for the above graph:

    \[
        A =
        \bordermatrix{
            & A & B & C \cr
            A & 0 & 1 & 0 \cr
            B & 0 & 0 & 0 \cr
            C & 1 & 1 & 0 \cr
        }
    \]

    Notice again how this first degree adjacency matrix only tells us if a \emph{direct} edge exists between any start node and terminal node. Now, pay attention to the following calculations we will perform on two different paths for the sake of example: $C \to B$ and $B \to C$.

    First, let's take $C \to B$. From the graph picture, we can see that there are two paths from $C \to B$. There is one direct path from $C$ to $B$, which is why $A_{(C, B)} = 1$. Additionally, there is another path through intermediate node $A$, which corresponds to the path $C \to A \to B$. Let's see what happens when we calculate $A^2_{(C, B)}$, involving the dot product between the vector formed by row $C$ and the vector formed by column $B$.

    \[
        u =
        \begin{bmatrix}
            1 & 1 & 0
        \end{bmatrix}
        \quad
        v =
        \begin{bmatrix}
            1 \\
            0 \\
            1
        \end{bmatrix}
    \]

    Like we mentioned earlier in section~\ref{sec:3.1}, in this format, $u$ represents the existence of a direct path that begins at node $C$ and ends at every other node in the graph, while $v$ represents the existence of a direct path that begins at every other node in the graph and ends at node $B$. We can think of each product in this dot product as if asking the question ``Is there a \emph{direct} path from $C$ to some node $k$ and then is there a \emph{direct} path from that node $k$ to $B$?'' If the answer is yes to both parts of that question, this individual product amounts to 1. Below is the dot product calculation.

    \[
        u \cdot v = (1 \cdot 1) + (1 \cdot 0) + (0 \cdot 1) = 1
    \]

    You can see how the dot product considers every possible choice of intermediate node, and the result of the dot product should be the number of paths from $C$ to $B$ using exactly one intermediate node. Below is the completed product matrix $A^2$.

    \[
        A^2 =
        \bordermatrix{
            & A^2 & B^2 & C^2 \cr
            A^2 & 0 & 0 & 0 \cr
            B^2 & 0 & 0 & 0 \cr
            C^2 & 0 & 1 & 0 \cr
        }
    \]

    As you can see, if a path starting from $s$ and ending at $t$ exists with exactly one intermediate node, then that position will have a 1 in the matrix. By the same inductive reasoning we used in section~\ref{sec:3.1}, for an adjacency matrix of power $A^n$, the $A^n_{(s, t)}$ entry will track the number of paths from $s$ to $t$ on exactly $n - 1$ intermediate nodes.

    % Now, let's take $B \to C$. There exists no \emph{direct} path in our directed graph from $B \to C$, so we notice that the entry $A_{(B, C)} = 0$. When we square $A$, the entry $A^2_{B, C}$ should contain a 1 if there is a possible path $B \to k \to C$ that takes one intermediate vertex. However, notice that there are no possible paths starting at $B$ and ending at $C$, so we expect $A^2_{(B, C)} = 0$. To calculate this entry, we take the dot product of the vector formed by row $B$ and the vector formed by column $C$.

    % \[
    %     u =
    %     \begin{bmatrix}
    %         0 & 0 & 0
    %     \end{bmatrix}
    %     \quad
    %     v =
    %     \begin{bmatrix}
    %         0 \\
    %         0 \\
    %         0
    %     \end{bmatrix}
    % \]

    % Here, $u$ represents the existence of paths starting at $B$ and ending at any other node in the graph, and $v$ represents the existence of paths starting from any other node in the graph and ending at $C$. Since it is obvious from the graph picture that $B$ has an out-degree of 0 and $C$ has an in-degree of 0, vectors $u$ and $v$ are unremarkably 0 vectors. Therefore, their dot product is also 0 and we find that $A^2_{(B, C)} = 0$.

    It might be self-explanatory why paths that cannot be formed with any number of intermediate nodes, for example $B \to C$ will have $A^n_{(B, C)} = 0 \quad \forall n \in \mathds{Z}^+$.
\end{quote}

\subsubsection{Matix Series Identity}

Let's try something new now. What happens if we want to form the accumulated adjacency matrix $A^{\prime}$ where $A^{\prime}_{(s, t)}$ tracks the total number of $s$-$t$ paths that can take any number of intermediate nodes? A straightforward thing to do would be to consider the sum of matrices of powers counting from 0 to $h$, since each power represents the number of $s$-$t$ paths on exactly $power - 1$ intermediate nodes.

\[
    A^{\prime} = A^0 + A^1 + A^2 + A^3 + \cdots
\]

Does this look familiar to you? You might notice that this sum has a similar format to the infinite geometric series. Note that $A^0 = I$, and the identity matrix adds 1's along the diagonal, which simply adds circular edges from each node to itself. This does not impact any calculations.

We know that a formal geometric series of the form $1 + x + x^2 + \cdots$ can be represented by the generating function

\[
    \frac{1}{1 - x} = 1 + x + x^2 + \cdots
\]

Interestingly, under the conditions of adjacency matrices representing directed acyclic graphs (\texttt{DAG}s), or over a field with random weights, we can use a ``generating function'' to efficiently compute $A^{\prime}$ as well! In particular, we can use the below identity:

\[
    (I - A)^{-1} = A^0 + A^1 + A^2 + A^3 + \cdots
\]

Now, by computing the matrix $A^{\prime} = (I - A)^{-1}$, we have a new matrix we can use to check the existence of an $s$-$t$ path. The $A^{\prime}_{(s, t)}$ entry will be non-zero if a path exists from $s$ to $t$ on any number of intermediate nodes, or $0$ if otherwise. To understand this identity rigorously, we can recall our abstract algebra concepts from section~\ref{sec:3.2}. Notably, our identity only works for \texttt{DAG}s and field matrices since we need a genuine multiplicative inverse to be defined. In a field, every nonzero scalar has an inverse, and determinants live there, so you can talk about $\det(I - A)\neq 0$ and hence $(I - A)^{-1}$ exists.  In an arbitrary ring (say \(\mathbb Z\) or a semiring), you can't guarantee that. Additionally, we need the series $A^0 + A^1 + A^2 + A^3 + \cdots$ to eventually stop since an infinite sum of matrices does not have much meaning unless the tail vanishes. On a \texttt{DAG}, we know the adjacency matrix $A^n = 0$ since there are no paths of length greater than or equal to $n$, as the longest path is of length $n - 1$. However, we will be covering how we can adapt some of these restrictions to general commutative rings so that our algorithm can cover a broader range of matrices.

\subsubsection{Polynomial Matrices as the Next Intuitive Step} \label{sec:3.3.3}

So where do polynomial matrices come into the picture? As discussed previously, our accumulated adjacency matrix $A^{\prime}$ can track the number of $s$-$t$ paths on any number of intermediate nodes. However, just the total number of $s$-$t$ paths does not help us solve \texttt{APSP} since we need some mechanism to determine the \emph{shortest} $s$-$t$ path. An intelligent mechanism to do just this would be to make use of \emph{indeterminates} where we can collect the number of $s$-$t$ paths taking $n$ intermediate nodes as the coefficient of the $x^n$ term of a polynomial.

Let's formalize this theory. Instead of the geometric series we saw earlier, we can instead represent $A^{\prime}$ using a formal power series; we'll call this new matrix $A^{\prime \prime}$:

\[
    A^{\prime \prime} = A^0 x^0 + A^1 x^1 + \cdots + A^n x^n
\]

We can use a similar identity to the one we came up with for $A^{\prime}$ to represent $A^{\prime \prime}$.

\[
    A^{\prime \prime} = (I - xA)^{-1}
\]

We will show why this identity works in subsection~\ref{sec:3.3.4}, but for now, let's see a simple example taking it for granted.

\begin{quote}
    Consider the following graph:

    \begin{center}
        \includegraphics[scale=0.5]{media/Figure_3.png}
    \end{center}

    With the adjacency matrix below:

    \[
        A =
        \bordermatrix{
            & a & b & c \cr
            a & 0 & 1 & 0 \cr
            b & 1 & 0 & 1 \cr
            c & 1 & 0 & 0 \cr
        }
    \]

    We compute $A^{\prime \prime}$ as $(I - xA)^{-1}$:

    \[
        A^{\prime \prime} =
        \left(
            \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                0 & 0 & 1
            \end{bmatrix}
            -
            x
            \begin{bmatrix}
                0 & 1 & 0 \\
                1 & 0 & 1 \\
                1 & 0 & 0
            \end{bmatrix}
        \right)^{-1}
        =
        \left(
            \begin{bmatrix}
                1 & -x & 0 \\
                -x & 1 & -x \\
                -x & 0 & 1
            \end{bmatrix}
        \right)^{-1}
    \]

    $A^{\prime \prime}$ can be simplified to:

    \[
        A^{\prime \prime} =
        \begin{bmatrix}
            x^2 + 1 & x & x^2 \\
            x^2 + x & x^2 + 1 & x \\
            x & x^2 & 1
        \end{bmatrix}
    \]

    We will cover why the above matrix corresponds to the inverse of $(I - xA)$ in the section~\ref{sec:3.3.4}, but for now, notice how the \emph{coefficient} of the $x^n$ term of each entry $A^{\prime \prime}_{(s, t)}$ indicate the number of $s$-$t$ paths of length $n$. It may occur to you that if we can find an algorithm to efficiently compute $A^{\prime \prime}$ for a given graph, we can solve \texttt{APSP} by simply looking at the lowest degree term of each $A^{\prime \prime}_{(s, t)}$ entry with a non-zero coefficient.
\end{quote}

The example above shows us what we can expect from the matrix $A^{\prime \prime}$, but before we can take the results for granted, we must discuss the details of polynomial matrices make mathematical sense.

\subsubsection{How are Polynomial Matrices Defined?} \label{sec:3.3.4}

Before we get ahead of ourselves, let's understand how exactly we can even define matrices over polynomials and how we can expect them to have inverses. To understand this properly, we will be referring to the abstract algebra concepts we defined in section~\ref{sec:3.2}.

We traditionally think of matrices as algebraic structures that contain numbers, but in reality we just make the assumption that the matrices we encounter are defined over a \emph{ring} $R$ that contains the real numbers. To pass from ``numeric'' matrices to polynomial matrices, we must consider defining matrices over the ring $R[x]$, which replaces each scalar in $R$ with a polynomial in the indeterminate $x$.

Let's be a little more precise in what we mean. For our matrix $A^{\prime \prime}$, we notice that any entry $A^{\prime \prime}_{(s, t)}$ is of the form

\[
    A^{\prime \prime}_{(s, t)} = \alpha_0 x^0 + \alpha_1 x^1 + \cdots
\]

where $\alpha_0, \alpha_1, \ldots$ are \emph{integers} in $\mathds{Z}$. We can specifically define $R[x]$ as a ring that suits this purpose. In particular:

\begin{enumerate}
    \item $R[x]$ is defined over the set of polynomial with integer coefficients, which is represented as $\mathds{Z}[x]$
    \item $(R[x], \oplus)$ is defined as typical polynomial addition where we can combine like terms
    \item $(R[x], \otimes)$ is defined as typical polynomial multiplication, which involves convolving the coefficients of each polynomial
\end{enumerate}

While this definition of $R[x]$ allows us to define polynomial matrices, we still cannot assume that a polynomial matrix has an inverse, which we use in our $A^{\prime \prime}$ identity. Recall earlier where we specifically defined the identity for $A^{\prime}$ to only work over \emph{fields}, where every non-zero scalar was defined to have a multiplicative inverse and we also assumed that we only deal with \texttt{DAG}s to get rid of the infinite matrix sum. While we cannot make the same assumptions for a general commutative ring like $R[x]$, we can show that under the below listed conditions, the matrix $A^{\prime \prime} = (I - xA)^{-1}$ always entries that have a multiplicative inverse and are finite series.

\begin{enumerate}
    \item To make sure that all the entries of $A^{\prime \prime}$ are finite, we can define all polynomial arithmetic to consider only polynomial modulo $x^h$. Specifically, we will redefine $R[x]$ as $\mathds{Z}[x] / \langle x^h \rangle$. When we talk about a polynomial modulo $x^h$, we simply mean that we truncate all of the polynomial terms that have a degree greater than $h$.

        {\footnotesize A size effect of considering only $\mathds{Z}[x] / \langle x^h \rangle$ is that we won't be able use $A^{\prime \prime}$ to track paths of length greater than $h$. We will discuss this more in section~\ref{sec:3.5} and section~\ref{sec:4}}

    \item In the ring $R[x] = \mathds{Z}[x] / \langle x^h \rangle$, the polynomial $p(x)$ is a unit (i.e. invertible) if and only if its constant term is a unit in $\mathds{Z}$. In other words,

        \[
            p(x) = \alpha_0 x^0 + \alpha_1 x^1 + \cdots + \alpha_{h - 1} x^{h - 1}
        \]

        and we notice that there cannot be a $q(x)$ such that $p(x) \cdot q(x) \equiv 1 \mod x^h$ if $\alpha_0$ is not invertible in $\mathds{Z}$. Since we defined $(A^{\prime \prime})^{-1} = (I - xA)$, entries along the diagonal of $(I - xA)$ must have a non-zero constant term. We may notice in this matrix that the identity matrix guarantees constant terms of 1 along the diagonal, so we can confidently say that $(I - xA)$ always has an inverse in $\mathds{Z}[x] / \langle x^h \rangle$. It can further be proven that a matrix of the form $(I - xA)$ has an inverse in \emph{any} ring, but we'll spare you the detailed proof.
\end{enumerate}

Due to the conditions we enforce above, we can guarantee that $A^{\prime \prime}$ exists. If you recall from section~\ref{sec:3.3.3}, we magically computed the inverse of an example $(I - xA)$ matrix. Using our knowledge of the ring $\mathds{Z}[x] / \langle x^h \rangle$, we can show the work for computing the inverse below:

\begin{quote}
    \[
        (I - xA) =
        \begin{bmatrix}
            1 & -x & 0 \\
            -x & 1 & -x \\
            -x & 0 & 1
        \end{bmatrix}
        \qquad
        (I - xA)^{-1} =
        \begin{bmatrix}
            x^2 + 1 & x & x^2 \\
            x^2 + x & x^2 + 1 & x \\
            x & x^2 & 1
        \end{bmatrix}
    \]

    Let's compute $(I - xA)^{-1} (I - xA)$ and show that it equals the identity matrix:

    \[
        \begin{split}
            (I - xA)^{-1} (I - xA) &=
            \begin{bmatrix}
                1 - x^3 & -x^3 & 0 \\
                -2x^3 & -x^3 + 1 & -x^3 \\
                -x^3 & 0 & -x^3 + 1
            \end{bmatrix} \\
            &=
            \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                0 & 0 & 1
            \end{bmatrix}
            \mod x^h
        \end{split}
    \]

    If we take $h = 3$, we truncate all terms of each polynomial of degree $h$ or greater. Therefore, you can notice how $(I - xA)^{-1} (I - xA)$ is actually the identity matrix.
\end{quote}

It might occur to you that the simplest way to compute $A^{\prime \prime}$ would be to simply take the sum of the first $h$ powers of $A$. To compute every next power of $A$, $A^{i + 1} = A^i \times A$, so we will require a total of $h$ matrix multiplications that take $\bigO(n^3)$ time each naively. Therefore, we can say to initialize the data structure that holds $A^{\prime \prime}$, we require $\bigO(h n^3)$ time. Doesn't this seem rather inefficient compared to just using the Floyd-Warshall algorithm in $\bigO(n^3)$ time? Well, the benefit of using this data structure is that we can more efficiently support computing \texttt{APSP} on graphs that \emph{change} after initialization. This is the essence of dynamic algorithms, as we tradeoff potentially heavier initialization costs for lighter costs to compute changes. We will explore how exactly we achieve better update efficiency in the next section.

Now that we have covered the fundamentals of polynomial matrices, in the next sections, we will talk about intelligent ways to modify them dynamically, allowing us to conclude how we might arrive at Jan van den Brand's algorithm.

\subsection{Edge Updates} \label{sec:3.4}

As we mentioned at the end of the previous section, the real benefit of using a polynomial matrix as our adjacency matrix in terms of time efficiency is that we can more effectively track \emph{changes} in \texttt{APSP} for \emph{changing} graphs as opposed to a simple re-running of the Floyd-Warshall algorithm. Let's explore exactly how we can be more efficient:

\begin{quote}
    We want to accomplish a time complexity that is better than $\bigO(n^3)$, which is the Floyd-Warshall time complexity, when we update our graph $G$ by either adding or removing nodes. When representing a graph $G$ as an adjacency matrix $A$, we realize that we can add or remove a direct edge by modifying the entry in $A_{(s, t)}$. One way we can mathematically represent this type of modification is through the expression:

    \[
        A + u \cdot v^{\top}
    \]

    where one of $u$ or $v$ is an elementary column vector $e_i$, which is a vector of all 0's and a 1 in the $i$-th position. It is important to note that $u$ and $v$ are both \emph{column} vectors (i.e they have the dimension of $n \times 1$, hence $n \times 1$ vector multiplied by a $1 \times n$ vector is an $n \times n$ matrix). You can see how if we set $u = e_i$, the \emph{row} vector $v^{\top}$ replaces the $i$-th row of $A$. Setting $v = e_i$ replaces the $i$-th column of $A$ with the \emph{column} vector $u$. We call these types of updates as \emph{rank-one} updates.

    Why is this notation particularly useful? We can use an identity called the Sherman-Morrison Identity to efficiently calculate $A^{\prime \prime}$ using rank-one updates. Below is the Sherman-Morrison Identity:

    \[
        \begin{split}
            (M + u v^{\top})^{-1} &= M^{-1} - \frac{M^{-1} u v^{\top} M^{-1}}{1 + v^{\top} M^{-1} u} \\
            &= M^{-1} u (1 + v^{\top} M^{-1} u)^{-1} v^{\top} M^{-1}
        \end{split}
    \]

    The derivation of this identity is not pivotal to understanding Jan van den Brand's algorithm, however, it is useful to understand why we can use this identity even though it requires a division which may not always be defined on a ring. In reality, we can invert $1 + v^{\top} M^{-1} u$ because the $M$ matrix is really just our $(1 - xA)$ matrix, which we provided an argument for being invertible in section~\ref{sec:3.3.4}. Let's extend the Sherman-Morrison Identity to our context to make this more obvious:

    We want to compute $A^{\prime \prime \prime} = (I - x(A + u v^{\top}))^{-1}$. Note that $M = (A^{\prime \prime})^{-1} = (I - xA)$.

    \begin{align*}
        A'''
        &= (I - x(A + u\,v^{\top}))^{-1} \\
        &= (I - xA - x\,u\,v^{\top})^{-1} \\
        &= (M - x\,u\,v^{\top})^{-1} \\[6pt]
        \text{Let}\quad U = x\,u,\quad V^{\top} = -\,v^{\top}, & \\[3pt]
        A'''
        &= (M + U\,V^{\top})^{-1} \\
        &\text{(by Sherman–Morrison)} \\[3pt]
        A'''
        &= M^{-1}
        - M^{-1}\,U\,(I + V^{\top}M^{-1}U)^{-1}\,V^{\top}M^{-1} \\[6pt]
        M^{-1}U
        &= x\,M^{-1}u,
        \quad
        V^{\top}M^{-1}
        = -\,v^{\top}M^{-1}, \\[3pt]
        V^{\top}M^{-1}U
        &= (-\,v^{\top})\,(M^{-1})\,(x\,u)
        = -\,x\,(v^{\top}M^{-1}u), \\[3pt]
        I + V^{\top}M^{-1}U
        &= I - x\,(v^{\top}M^{-1}u), \\[6pt]
        \therefore\quad
        A'''
        &= M^{-1}
        - \bigl(x\,M^{-1}u\bigr)
        \bigl(I - x\,(v^{\top}M^{-1}u)\bigr)^{-1}
        \bigl(-\,v^{\top}M^{-1}\bigr) \\[3pt]
        &= M^{-1}
        + x\,M^{-1}u
        \bigl(I - x\,(v^{\top}M^{-1}u)\bigr)^{-1}
        v^{\top}M^{-1}
    \end{align*}

    We can see that the computation for $A^{\prime \prime \prime}$ depends on four operations:

    \begin{enumerate}
        \item $M^{-1} U$, an $n \times n$ polynomial matrix times an $n$ vector. For each of the $n^2$ entries of $M$, you multiply two $h$-degree polynomials, which takes $\bigO(h \log h)$ time using the Fast Fourier Transform (\texttt{FFT}). This multiplication totally takes $\bigO(h n^2)$ (omitting a negligible factor of $\log h$).
        \item $V^{\top} M^{-1}$, similar to above, also takes $\bigO(h n^2)$.
        \item $(M^{-1} U)(V^{\top} M^{-1})$, the outer product of two length-$n$ polynomial vectors: $n^2$ polynomial multiplications (since $n \times 1$ multiplied by $1 \times n$) at $\bigO(h \log h)$ each, for $\bigO(h n^2)$.
        \item Inversion of the scalar polynomial $1 + V^{\top} M^{-1} U$ (degree $<h$): by repeated squaring, (similar to how we calculated the inverse of $A^{\prime \prime}$) and \texttt{FFT}-based multiplications takes $\bigO(h \log^2 h) \approx \bigO(h)$ (Since we have $\bigO(\log h)$ squares and a cost of $\bigO(h \log h)$ per square).
    \end{enumerate}

    Based on the above, we can see that computing $A^{\prime \prime \prime}$ using the Sherman-Morrison Identity takes $\bigO(h n^2 + h \log^2 h) = \bigO(h n^2)$ time, which beats $\bigO(n^3)$ from Dijkstra's depending on choice of $h$.
\end{quote}

It is important to observe that the polynomial-series coefficients can grow very large, making exact arithmetic expensive. To avoid this, we compute all coefficients modulo a large prime $p$, working over the finite field $\mathbb{Z}_p$. Of course, if the true number of $s$-$t$ paths is divisible by $p$, then the corresponding coefficient becomes zero modulo $p$, potentially masking reachability. However, by the Schwartz-Zippel lemma, any nonzero polynomial of degree at most $h$ can vanish modulo $p$ with probability at most
\[
    \frac{\deg(f)}{p} \;\le\;\frac{h}{p}.
\]
In particular, choosing $p>2h$ guarantees a failure probability of at most $\tfrac{1}{2}$. Moreover, if we repeat the construction independently for $O(\log n)$ different random primes $p_1,\dots,p_k$, then by a union bound the chance that any one of the $n^2$ entries is falsely zero in \emph{all} runs becomes negligible (e.g., $n^{-c}$ for any constant $c$). Hence, with high probability, no genuine reachability is lost.

\subsection{Hitting Sets} \label{sec:3.5}

In section~\ref{sec:3.4}, we covered how we can manipulate polynomial matrices to track \texttt{APSP} for path lengths up to length $h$ in $\displaystyle \tilde \bigO(h n^2)$ time per node update. One limitation of tracking the \texttt{APSP} using $A^{\prime \prime}$ from section~\ref{sec:3.3.3} is that we only track shortest $s$-$t$ paths of length $h$, since we assumed that all polynomial arithmetic would be limited by modulus $x^h$ on the ring $\mathds{Z}[x] / \langle x^h \rangle$. However, a shortest $s$-$t$ path may have a length that is greater than $h$, so how would we recover such paths without defaulting to Floyd-Warshall's algorithm that takes $\bigO(n^3)$ time?

To track the shortest paths of an arbitrary length $n$, we would need to find a way to deal with ``chunks'' of the path that are only of length $h$. A straightforward way to do this is to somehow subdivide the nodes of the graph $G$ into a new graph $H$ of $\frac{n}{h}$ nodes, where each node somehow is capable of representing a $\frac{n}{h}$ section of nodes from $G$. It turns out that we can subdivide the original graph $G$ by randomly sampling some $\frac{n}{h}$ nodes, with a high probability that shortest paths pass through the randomly sampled $\frac{n}{h}$ nodes of $H$.

Let's formalize this approach:

\begin{quote}
    We define the reduced graph $H(V, E^{\prime})$ as a graph with the following properties using only the $h$-bounded distances from $A^{\prime \prime}$:

    \begin{enumerate}
        \item $V$ is the same vertex set as in $G(V, E)$
        \item $E^{\prime}$ can be defined in the following manner:

            Define $R \subseteq V$ such that $|R| = \displaystyle \tilde \bigO(\frac{n}{h})$. We pick $R$ from $V$ uniformly at random.

            For \emph{every} $v \in V$ and \emph{every} $r \in R$, we add \emph{only} the following edges into $E^{\prime}$: $(1)$, we add the directed edge $v \to r$ with a weight of $\mathrm{dist}_{h-bounded}(v, r)$ and $(2)$, we add the edge $r \to v$ with a weight of $\mathrm{dist}_{h-bounded}(r, v)$. In simpler terms, we're building a ``hub-and-spoke'' mini-graph: for every node $v$ and every landmark $r \in R$, we're adding exactly two directed spokes\textemdash one $v \to r$ and the other $r \to v$\textemdash and we label each with the true distance in $G$.

            For the sake of being explicit, notice that $H$ has $|V| \times |R| \times \bigO(\frac{n}{h}) = 2 \times n \times \bigO(\frac{n}{h}) = \bigO(\frac{n^2}{h})$ edges.
    \end{enumerate}

    Below is a visualization for your convenience:

    \begin{center}
        \includegraphics[scale=0.4]{media/Figure_4.png}
    \end{center}

    With this structure for $H$, we can be confident that \emph{every} shortest path of a length greater than or equal to $h$ hits some $r \in R$ within each consecutive block of $h$ edges. In other words, every long $s$-$t$ path in $G$ can be ``stitched together'' in $H$ by hopping from one landmark node $r \in R$ to the next in steps guaranteed to be at most $h$ edges, so that the total cost in $H$ exactly matches the cost of the $s$-$t$ path in $G$.

    Why is this? Since we know $|R| = \displaystyle \tilde \bigO(\frac{n}{h})$, we can restate this as $|R| = \frac{p \cdot n}{h}$ for some $p \in \mathds{N}$.

    For a fixed $(s, t)$ whose shortest path has a length that is greater than or equal to $h$, we can say the probability that none of the first $h$ vertices lie in $R$ is:

    \[
        \left( 1 - \frac{|R|}{n} \right)^h \leq \left( 1 - \frac{\frac{p n}{h}}{n} \right)^h = \left( 1 - \frac{p}{h} \right)^h \leq e^{-p} = 2^{-\Omega(p)}
    \]

    {\footnotesize Recall, $(1 - \frac{x}{k})^k \leq e^{-x}$, and $e^{-p} = (2^{\ln e})^{-p} = (2^{\frac{1}{\ln 2}})^{-p}$, and $\Omega(p) = \frac{p}{\ln 2} \therefore e^{-p} = 2^{-\Omega(p)}$}

    There are at most $n^2$ choices for pairs of $(s, t)$, therefore, the probability that \emph{some} pair fails to ``hit'' $R$ is $n^2 \cdot 2^{\Omega(p)}$, which implies that the probability that every pair ``hits'' $R$ is $1 - \frac{n^2}{2^{-\Omega(p)}}$.

    Supposing that $\mathrm{dist}_G(s, t) \geq h$, we know the probability that we ``hit'' a single node in $R$ within $h$ nodes is $1 - \frac{n^2}{2^{-\Omega(p)}}$. Once we ``hit'' the first landmark node $r_0$, if the remaining length of the $s$-$t$ path is greater than or equal to $h$, we again face the probability of $1 - \frac{n^2}{2^{-\Omega(p)}}$ to ``hit'' another landmark node in $R$ within an additional $h$ nodes Therefore, we can reliably write any $s$-$t$ path as a dependency chain as follows: $s \to r_0 \to r_1 \to \cdots \to r_{i \leq \frac{n}{h}} \to t$.
\end{quote}

Above, we saw how we can convert our original graph $G$ with $n$ nodes into a reduced graph $H$ with $\frac{n}{h}$ landmark nodes which are strongly connected to all non-landmark nodes. Just to be precise, remember that while $H$ also has $n$ nodes like $G$, our landmark nodes are in the set $R \subseteq V$ and $|R| = \displaystyle \tilde \bigO(\frac{n}{h})$. Now, let's discuss precisely how we can recover $s$-$t$ paths of length greater than or equal to $h$ using our reduced graph $H$.

We know that $\mathrm{dist}_G(s, t) = \min \{\mathrm{dist}_{h-bounded}(s, t), \mathrm{dist}_H(s, t)\}$, since either there is a smaller than $h$-length $s$-$t$ path already present in $A^{\prime \prime}$ or we need to use our reduced graph $H$ to compute the distance. To compute the $s$-$t$ path distance on $H$, we can simply use Dijkstra's algorithm to solve \texttt{APSP} for $H$. This has a time complexity of $\bigO(|V| \times |E^{\prime}) = \bigO(n \times \frac{n^2}{h}) = \bigO(\frac{n^3}{h})$.

In section~\ref{sec:3.4}, we saw how we can achieve a time complexity of only $\bigO(h n^2)$ to update our $A^{\prime \prime}$ data structure. When we update our reduced graph $H$, we only need to update the edges from the new node $v$ to each $r \in R$ and from each $r \in R$ to $v$. This takes $\bigO(|R|) = \bigO(\frac{n}{h})$ time. Since we need to follow this up with an additional run of Dijkstra's algorithm, we can see the total update cost for $H$ is $\bigO(|V| \times |R|) = \bigO(n \times \frac{n}{h}) = \bigO(\frac{n^2}{h})$ time.

\section{Solution} \label{sec:4}

Using the technical background from section~\ref{sec:3}, we have everything we need to go through the dynamic \texttt{APSP} algorithm from Jan van den Brand's notes.

Let's walk through the algorithm start to finish! Let's start with maintaining the $h$ bounded shortest path distances. These are the distances between node pairs where $h$ is the maximum length of the shortest path. We use the inverse of the polynomial matrix, $A^{\prime \prime} = (I - xA)^{-1}$, over a truncated ring $R[x] = \mathds{Z}[x] / \langle x^h \rangle$ to do this, where $A$ is the adjacency matrix of the graph, and the minimum degree of $x$ in an entry $A^{\prime \prime}_{(s, t)}$ represents the shortest distance between nodes $s$ and $t$. The Sherman-Morrison identity is then used to modify the inverse matrix, $A^{\prime \prime}$, for updates that affect edges incident to a single vertex.

The next part involves extending the $h$-bound pair distances to cover distances of arbitrary length $n$. We do this using hitting sets, where we randomly pick $\frac{n}{h}$ nodes from the original graph. This is based on the idea that any long path is highly likely to pass through at least one of the randomly sampled nodes. We then construct a reduced graph $H$, where each node is connected to the sampled nodes using the short distances from before up to length $h$.

Finally, we run Dijkstra's algorithm for each node $v$ on the auxiliary graph to compute the shortest path between $v$ and every landmark node $r$. We store the computed \texttt{APSP} on $H$.

In the end, we have two pieces of information to work with. We have the $h$-bounded distances in $A^{\prime \prime}$, represented by $\mathrm{dist}_{h-bounded}(s, t)$, which are exact for the paths of length $\leq h$. We also have the reduced graph distances stored in $H$, represented by $\mathrm{dist}_H(s, t)$, which are accurate for longer paths greater than $h$. For each node pair $(s, t)$, we return $\min({\mathrm{dist}_{h-bounded}(s, t), \mathrm{dist}_H(s, t)})$ as $\mathrm{dist}_G(s, t)$.

To analyze the time complexity of this algorithm, we can analyze the time complexities related to the two data structures we maintain: $A^{\prime \prime}$ and $H$:

\begin{enumerate}
    \item We need to initialize our data structure from \textbf{theorem 1.0.0}, which is essentially our matrix $A^{\prime \prime} = (I - xA)^{-1}$ from section~\ref{sec:3.3.3}, combined with our reduced graph $H$ mentioned in section~\ref{sec:3.5}. As we mentioned at the end of section~\ref{sec:3.3.4}, it takes $\bigO(h n^3)$ time to initialize $A^{\prime \prime}$, and taking $h = \sqrt{n}$ as mentioned in section~\ref{sec:3.4}, we can accomplish this in $\bigO(n^{3.5})$ time. Additionally, as mentioned at the end of section~\ref{sec:3.5}, it takes $\bigO(\frac{n^3}{h})$ time to initialize $H$, and taking $h = \sqrt{n}$, we find it takes $\bigO(n^{2.5})$ time. In total, initialization takes $\bigO(n^{3.5} + n^{2.5}) = \bigO(n^{3.5})$ time, as desired from the theorem.
    \item We need to update our data structure mentioned in the previous part. As mentioned in section~\ref{sec:3.4}, we can accomplish adding and removing an edge from $A^{\prime \prime}$ using a \emph{rank-one} update in $\bigO(h n^2)$, which taking $h = \sqrt{n}$, is $\bigO(n^{2.5})$ time. In order to update our reduced graph $H$, we take $\bigO(\frac{n^2}{h})$ time as mentioned at the end of section~\ref{sec:3.5}. Again, taking $h = \sqrt{n}$, we get an update time of $\bigO(n^{1.5})$. In total, we have an update time complexity of $\bigO(n^{2.5} + n^{1.5}) = \bigO(n^{2.5})$, as desired from the theorem.
\end{enumerate}

Looking at the time complexities, it is obvious that the Floyd-Warshall algorithm is more efficient in initialization costs, as it takes $\bigO(n^3)$ to initialize compared to the $\bigO(n^{3.5})$ that Jan van den Brand's algorithm takes. However, Jan van den Brand's algorithm is more efficient when dealing with updating edges in the graph $G$, as we can effectively update \texttt{APSP} in $\bigO(n^{2.5})$ time whereas the Floyd-Warshall algorithm would have to be re-run, taking $\bigO(n^3)$ time again. We can see how Jan van den Brand's algorithm prioritizes dynamic update costs while the Floyd-Warshall algorithm prioritizes static cost.

\section{Extension and Comparison}

In this section, we will briefly analyze differences between Jan van den Brand's algorithm which we corresponded to \textbf{theorem 1.0.0} and Sankowski's algorithm that we corresponded to \textbf{theorem 1.0.1}. Sankowski's algorithm primarily differs from Jan van den Brand's algorithm in how it conceptualizes adjacency matrices, which leads it to have more groundbreaking time complexities.

\subsection{Broader Algebraic Structures}

Sankowski's paper works directly with general commutative rings where division might not be defined or possible. The paper develops techniques using formal power series that avoid the need for division operations, expressing matrices as $A(u)$ = $I + u(A - I)$ and working in the ring of formal power series $R[[u]]$. This makes his approach applicable to a much wider range of mathematical structures (not limited to field structures particularly). In contrast, Jan van den Brand's algorithm restricts itself to contexts where polynomial division is possible, essentially requiring a field structure rather than just a ring, limiting its mathematical generality.

{\footnotesize Even though we mentioned earlier that Jan van den Brand's algorithm works on general commutative rings, remember that we enforced certain conditions on the ring we used (i.e. polynomials modulo $h$, and we only took the inverse for our defined $A^{\prime \prime}$ matrix).}

Sankowski's more general framework also provides a way to compute both determinant and matrix adjoint over a commutative ring. The adjoint approach is particularly significant as it works without requiring the matrix to be invertible, making it more robust. While we won't go into the details here, there is a significant relationship between the matrix adjoint and the matrix inverse which is pivotal to understanding this approach. As we know, Jan van den Brand's algorithm simplifies by focusing only on matrix inverse operations (this can be considered as working on the polynomial ring), losing some of the mathematical power and generality of Sankowski's approach, which can leverage different matrix functions depending on the specific problem requirements.

\subsection{General Update Query}

Looking at \textbf{Theorem 3.0.1} from the Jan van den Brand's notes, we see it describes a data structure with two main operations. First, an \textsc{Initialize} operation that works on an $n$-node graph and returns \texttt{APSP} in $O(n^{3.5})$ time. Second, an \textsc{Update} operation that handles changes to edges connected to a single vertex $v$ and returns \texttt{APSP} in $O(n^{2.5})$ time.

In comparison, \textbf{Theorem 8} from Sankowski's original work shows a much more powerful result. It presents a randomized algorithm for the dynamic \texttt{APSP} problem in unweighted graphs that supports edge updates in $O(n^{1.932})$ time and queries in $O(n^{1.288})$ time. This is significantly faster than the $O(n^{2.5})$ time mentioned in \textbf{Theorem 3.0.1}.

The proof of \textbf{Theorem 8} maintains several data structures:
\begin{itemize}
    \item A randomly chosen set $H$ of vertices.
    \item An $n\times n$ matrix $D$ capturing shortest paths using at most $n^\mu$ edges.
    \item A smaller matrix $B$ extracted from $D$, containing only the rows and columns corresponding to vertices in $H$.
    \item The Kleene closure $B^*$ of the matrix $B$.
\end{itemize}
During updates, the matrix $D$ is updated using an algorithm from \textbf{Theorem 6}, taking
\[
    O\bigl(n^\mu \bigl(n^{1+\varepsilon} + n^{\omega(1,\varepsilon,1)-\varepsilon}\bigr)\bigr)
\]
time.

\paragraph{Theorem 6.}
There exists an algorithm for maintaining dynamic shortest distances up to length $k$ in an unweighted directed graph with update time
\[
    \tilde O\bigl(k\,(n^{\omega(1,\varepsilon,1)-\varepsilon} + n^{1+\varepsilon})\bigr)
    \quad\text{and}\quad
    \tilde O(k\,n^\varepsilon)
\]
query time. The core idea is to interpret the truncated power-series inverse
\[
    (I - uA)^{-1} \bmod u^{k+1}
    \;=\;
    \sum_{i=0}^{k} u^i\,A^i
\]
over the ring $\mathbb F[[u]]/\langle u^{k+1}\rangle$.  The coefficient of $u^i$ in entry $(s,t)$ counts the number of length-$i$ paths, so the smallest $i$ with nonzero coefficient is exactly the $s\!t$ distance bounded by $k$.  One then plugs this into a dynamic-matrix-inverse data structure over formal series and applies lazy update techniques to achieve the stated bounds.

\paragraph{Theorem 8.}
Combining Theorem 6 with a random-landmark decomposition yields a fully subquadratic algorithm for unbounded dynamic shortest-path queries in unweighted graphs, with expected update time $O(n^{1.932})$ and query time $O(n^{1.288})$.  Concretely, one maintains:
\begin{enumerate}
    \item A random landmark set $H\subset V$ of size $\tilde O(n^{1-\mu})$.
    \item An $n\times n$ distance matrix $D$, accurate for paths of length $\le n^\mu$ (via Theorem 6).
    \item The restricted $|H|\times|H|$ submatrix $B=D|_{H\times H}$ and its Kleene closure $B^*$.
\end{enumerate}
On each edge update, refreshing $D$, $B$, and recomputing $B^*$ costs
\[
    \tilde O\!\bigl(n^\mu(n^{1+\varepsilon}+n^{\omega(1,\varepsilon,1)-\varepsilon}) \;+\; n^{1+\mu}\;+\;n^{3-3\mu}\bigr),
\]
and choosing $\mu\approx0.357,\;\varepsilon\approx0.575$ balances these to $O(n^{1.932})$.  A query then returns
\[
    \min\!\bigl\{D_{s,t},\,\min_{p,q\in H}\!(D_{s,p}+B^*_{p,q}+D_{q,t})\bigr\}
\]
in $O(n^{1.288})$ time.

\paragraph{Summary.}
The small landmark core $B$ and its closure $B^*$ capture all long-range interactions among a sparse set of vertices, allowing “bridging” beyond the $k$-hop approximation in $D$ without ever paying $\Omega(n^2)$ per update.

This shows why Sankowski's paper is more general and efficient. It not only handles both edge updates and node updates (as in Theorem 3.0.1), but does so with better time complexity. The comparison with Floyd-Warshall\textemdash which would require $O(n^3)$ time per update\textemdash further highlights how both approaches improve on the trivial solution, while Sankowski's original work achieves much better performance with its subquadratic time complexities.

\section*{References}

\begin{enumerate}
    \item \href{https://www.dropbox.com/scl/fi/zjfbk8dbxbzcn05dr8l11/fall22_daa_lecturenotes.pdf?rlkey=g6z7z1hvj7jknvlpy1s3dqcjw&e=1&st=13r18doq&dl=0}{Jan van den Brand's Notes}
    \item \href{https://www.dropbox.com/scl/fi/5w4k0lg0u8e36zugapp6c/Sankowski-COCOON-05-subquadratic-dynamic-distances.pdf?rlkey=5eahwsl7a49kwxyvlr7yqyoz3&e=2&st=d3zo7owc&dl=0}{Sankowski's Paper}
\end{enumerate}

\end{document}
