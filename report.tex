% --------- document setup ---------
\documentclass[12pt]{article}
\usepackage[
    left=1in,
    right=1in,
    top=0.5in,
    bottom=0.5in,
    includehead,
    includefoot
]{geometry}
\setlength{\parskip}{0.75em} % larger jumps at end of paragraph
\setlength{\parindent}{0pt} % no indent
\pagenumbering{gobble} % no page numbers

% --------- packages ---------
\usepackage{graphicx} % required for inserting images
\usepackage{multicol} % two column feature
\usepackage{hyperref} % hyperlinks
\usepackage{etoolbox} % if-then-else logic
\usepackage{amsmath, amsfonts, amssymb, amsthm, mathrsfs} % math
\usepackage{float} % figure env
\usepackage{tikz} % drawing things
\usetikzlibrary{automata, positioning}
\usepackage{xcolor} % colors
\usepackage[most]{tcolorbox} % answer boxes
\usepackage{fancyhdr} % header
\usepackage{fancyvrb} % fancy verbatim
\usepackage{ifthen} % conditional statements
\usepackage{lipsum} % filler text
\usepackage{xstring} % For string manipulation
\usepackage{enumitem} % for hint hanging indent
\usepackage{outlines} % for outlines
\usepackage{fancybox} % for nice title page box
\usepackage{soul} % for strikethrough
\usepackage{algorithm} % for algorithms
\usepackage{dsfont}

\newcommand{\note}[1]{{\footnotesize
        \begin{description}
                [leftmargin=3.4em,style=nextline]
            \item[Note:] {#1}
        \end{description}}
}

% --------- Header + Title ---------
\renewcommand\maketitle{
    \begin{center}
        \shadowbox{\parbox{5.3in}{{\small {\bf {\sc Georgia Tech Spring 2025
                \hfill CS 3511: Honors Algorithms}}}
                \medbreak
                \begin{center}
                    {\large \bfseries \reportTitle}
                \end{center}
                \medbreak
                Authors: {\em \authors}
                \newline
                Date: \reportDate
            \vspace{2pt}}
        }
        \vspace{6pt}
    \end{center}
}

% --------- Custom symbols ---------
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\Ptime}{\mathrm{P}}
\newcommand{\NPtime}{\mathrm{NP}}
\newcommand{\poly}{\text{poly}}
\newcommand{\Alg}{\mathcal{A}}
\newcommand{\Opt}{\operatorname{OPT}}
\newcommand{\dx}{\ensuremath{\mathrm{d}x}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor} % for floor
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil} % for ceiling

% ------------ Document ------------
\begin{document}

\newcommand{\reportTitle}{Final Project}
\newcommand{\reportDate}{Wednesday 4/16/25}

\newcommand{\authors}{Arjun Sheshadri, Vilohith Gokarakonda, Yiqi Sun, Tyler Huang}

\maketitle

\section{Introduction}

Suppose if you have a graph $G = (V, E)$ with a given adjacency matrix, which represents where any two nodes are connected to each other. Suppose you wanted to find the shortest path to one node, using all nodes as a starting point.

Shortest distances between all pairs of nodes in a graph is important for its real world applications in communication networks, social graphs, and more. Edges in such graphs can be added and removed at any time, so being able to efficiently maintain all pairs distances is crucial. Using the Floyd-Warshall Algorithm, we are able to accomplish this in $\bigO(n^3)$ time. However, this becomes inefficient for larger or frequently changing graphs.

This report explores a \textit{dynamic algorithm} described in Jan van den Brand's \href{https://www.dropbox.com/scl/fi/zjfbk8dbxbzcn05dr8l11/fall22_daa_lecturenotes.pdf?rlkey=g6z7z1hvj7jknvlpy1s3dqcjw&e=1&st=13r18doq&dl=0}{notes} for maintaining All-Pairs-Shortest-Paths (\texttt{APSP}) in directed graphs with or without edge weights. Instead of recalculating shortest distances from scratch every update, it efficiently maintains a data structure where all distances can be updated in $\displaystyle \tilde \bigO(n^{2.5})$ time when an edge to a single vertex is added or removed. Specifically, it uses concepts from dynamic algebraic algorithms, involving polynomial matrix inverses to represent path information, and extending them to full distances with random sampling and Dijkstra's algorithm.

In this report, we present the problem statement, and the technical background relevant to the paper, including ring algebra, polynomial matrices, and how edges are updated. We also cover the steps to the solution, and connect this approach to the original groundbreaking \href{https://www.dropbox.com/scl/fi/5w4k0lg0u8e36zugapp6c/Sankowski-COCOON-05-subquadratic-dynamic-distances.pdf?rlkey=5eahwsl7a49kwxyvlr7yqyoz3&e=2&st=d3zo7owc&dl=0}{paper} by Sankowski, which can do updates in $\displaystyle \tilde \bigO(n^{1.932})$  randomized time and queries in $\displaystyle \tilde \bigO(n^{1.288})$ randomized time. Specifically, we compare their mathematical foundations, update operations, applications, and time complexities.

\section{Problem Statement}

Given a graph $G(V, E)$, we want to develop a data structure that can maintain \texttt{APSP} dynamically with an initial overhead $\displaystyle \tilde \bigO(n^{3.5})$ time and supports queries and updates in $\displaystyle \tilde \bigO(n^{2.5})$ time. Since this algorithm is optimized for maintaining \texttt{APSP} for changing graphs (hence requires a dynamic algorithm), having a time complexity of $\displaystyle \tilde \bigO(n^{2.5})$ is more efficient compared to the naive approach which takes $\bigO(n^3)$ time for queries and updates.

\newpage

Specifically, we will be working towards proving the following theorem:

\textbf{Theorem 1.0.0}: \emph{There exists a data structure that supports the following operations}:
\begin{enumerate}
    \item \textsc{Initialize($G = (V, E)$)} \emph{Initialize an $n$-node graph and return \texttt{APSP} in $\displaystyle \tilde \bigO(n^{3.5})$ time.}
    \item \textsc{Update($v,\,E^+,\,E^-$)} \emph{Given a vertex $v$ and two sets of edges $E^+\subseteq(\{v\}\times V\cup V\times\{v\})$ to insert and $E^-$ to delete (all incident to $v$), update $G$ and return the new \texttt{APSP} in $\displaystyle \tilde \bigO(n^{2.5})$ time.}
\end{enumerate}

Additionally, we will be extending this theorem using Sankowski's paper:

\textbf{Theorem 1.0.1}: \emph{There exists a data structure that supports the following operations}:
\begin{enumerate}
    \item \textsc{Initialize($G = (V, E)$)} \emph{Preprocess in $\displaystyle \tilde \bigO(n^{3})$ time.}
    \item \textsc{Update($e$)} \emph{Insert or delete a single edge $e\in V\times V$ in $\bigO(n^{1.932})$ randomized time.}
    \item \textsc{Query($s, t$)} \emph{Return the current distance $\mathrm{dist}_G(s,t)$ in $\bigO(n^{1.288})$ randomized time.}
\end{enumerate}
\emph{All operations succeed with high probability over the random choices.}

We will be covering general overviews of topics that help toward understanding the intuition behind these theorems in the next section.

\section{Technical Background}

\subsection{Matrix Multiplication Recap and Naive APSP} \label{sec:3.1}

Matrix multiplication is an important concept that we will be making extensive use of to understand the technical components in this report. You are already familiar with the basic method of multiplying two matrices with the same inner dimension, where you multiply the $n$-th row of the first matrix with the $n$-th column of the second matrix. Below is a visual representation:

\[
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
    \begin{bmatrix}
        e & f \\
        g & h
    \end{bmatrix}
    =
    \begin{bmatrix}
        a \cdot e + b \cdot g & a \cdot f + b \cdot h \\
        c \cdot e + d \cdot g & c \cdot f + d \cdot h
    \end{bmatrix}
\]

Notice how in each $(i, j)$ entry of the product matrix, we compute the \emph{dot product} of the vector that forms the $i$-th row of the first matrix and the $j$-th column of the second matrix. The \emph{dot product} is simply an operation on the entries of two vectors involving the typical $+$ and $\times$ operations.

Let's take a quick detour from matrix multiplication to talk about the well-known Floyd-Warshall algorithm for computing \texttt{APSP}. The key realization to understanding this algorithm is that between some start node $s$ and some terminal node $t$, the shortest path from $s \to t$ is the shorter path we can get from directly traveling from $s \to t$ vs. the path we get by traveling from $s$ to some intermediate node $k$ and then from $k$ to $t$. For a visualization, consider the trivial example below:

\begin{center}
    \includegraphics[scale=0.5]{media/Figure_1.png}
\end{center}

In the above graphic, the direct path from $s \to t$ has a cost of 11, while the path from $s \to k$ has a cost of 5 and the path from $k \to t$ has a cost of 5 as well. We can see that if we take $\min \{\mathrm{dist}_{s,t} = 11, \mathrm{dist}_{s,k} + \mathrm{dist}_{k,t} = 10 \}$, we get that by considering the intermediate node $k$, we arrive at a cheaper path from $s$ to $t$.

How does the above logic tie into matrix multiplication? Well, it might be intuitive to think of graphs in terms of adjacency matrices and the core Floyd-Warshall computation as a dot product between some vector $u$ that represents the shortest known paths from the starting node $s$ to all possible intermediate nodes $k$, and another vector $v$ that represents the shortest paths from all possible intermediate nodes $k$ to the terminal node $t$. Let's see an example of this analogy on the simple graph presented earlier:

\begin{quote}
    Consider the adjacency matrix $A$ for the graph presented earlier:

    \[
        A =
        \bordermatrix{
            & s & k & t \cr
            s & 0 & 5 & 11 \cr
            k & \infty & 0 & 5 \cr
            t & \infty & \infty & 0 \cr
        }
    \]

    Notice that in $A$, the $(i, j)$ entry represents the cost of the \emph{direct} path from node $i$ to node $j$. The rows and columns are labeled with their appropriate nodes on the borders.

    For the sake of computation, let's take the row $s$ and the column $t$ of the matrix as our $u$ and $v$ vectors. That is, let

    \[
        u =
        \begin{bmatrix}
            0 & 5 & 11
        \end{bmatrix}
        \qquad
        v =
        \begin{bmatrix}
            11 \\
            5 \\
            0
        \end{bmatrix}
    \]

    Just for clarity, recall that $u$ represents the shortest known path between the starting node $s$ and every other node $k$ in the graph. For column $s$ of $u$, the shortest path cost is 0 since we are already at $s$. For column $k$ of $u$, the path cost is 5 since the edge $s \to k$ on the graph has a cost of 5. For column $t$ of $u$, the path cost is 11 since the edge $s \to t$ has a cost of 11. In general, we see that each \emph{column} in $u$ represents the cost of the edge $s \to \textit{column in u}$. Following a similar logic, notice that in the vector $v$, each \emph{row} represents the cost of the edge $\textit{row in v} \to t$. Both \textit{column in $u$} and \textit{row in $v$} represent all the nodes in the graph.

    Let's replace the numbers in $A$ with symbols, and see what happens when we take the dot product of $u$ and $v$:

    \[
        A =
        \bordermatrix{
            & s & k & t \cr
            s & \mathrm{dist}_{s \to s} & \mathrm{dist}_{s \to k} & \mathrm{dist}_{s \to t} \cr
            k & \mathrm{dist}_{k \to s} & \mathrm{dist}_{k \to k} & \mathrm{dist}_{k \to t} \cr
            t & \mathrm{dist}_{t \to s} & \mathrm{dist}_{t \to k} & \mathrm{dist}_{t \to t} \cr
        }
    \]

    \[
        u =
        \begin{bmatrix}
            \mathrm{dist}_{s \to s} & \mathrm{dist}_{s \to k} & \mathrm{dist}_{s \to t}
        \end{bmatrix}
        \quad
        v =
        \begin{bmatrix}
            \mathrm{dist}_{s \to t} \\
            \mathrm{dist}_{k \to t} \\
            \mathrm{dist}_{t \to t}
        \end{bmatrix}
    \]

    Let's see what happens when we take the dot product of $u$ and $v$, which will be the $(s, t)$ entry of the matrix $A^2$:

    \begin{eqnarray}
        A^2_{(s,t)}
        &= \mathrm{dist}_{s \to s} \times \mathrm{dist}_{s \to t} \\
        &+ \; \mathrm{dist}_{s \to k} \times \mathrm{dist}_{k \to t} \\
        &+ \; \mathrm{dist}_{s \to t} \times \mathrm{dist}_{t \to t}
    \end{eqnarray}

    Based on the dot product formula, it seems that in the $(s, t)$ entry of the matrix $A^2$, we are able to calculate some information involving all paths from node $s$ to node $t$ involving one intermediate step through all the other nodes in the graph. Particularly, the product on line $(1)$ gives us information regarding the path $s \to s \to t$, the product on line $(2)$ gives us information regarding the path $s \to k \to t$, and the product on line $(3)$ gives us information regarding the path $s \to t \to t$. You may notice the paths $s \to s \to t$ and $s \to t \to t$ are simply the same as the path $s \to t$, a direct path from $s$ to $t$ with no intermediate nodes. It may become apparent to you that by induction, the matrix $(A^2)^2_{(s, t)}$ stores the shortest path between $s \to t$, $s \to k \to t$ for all graph vertices $k$, and $s \to l \to m \to t$ for all combinations of nodes $l$ and $m$ in the graph. We can say that the adjacency matrix $(A^2)^n_{(s, t)}$ stores the shortest path from $s$ to $t$ using \emph{up to} $n$ intermediate vertices.
\end{quote}

As shown in the above analogy, we are essentially able to store the result of a dot product operation between the path lengths of $s \to t$ and $s \to k \to t$ for every intermediate node $k$ in the graph. As we explained earlier during our discussion of Floyd-Warshall's algorithm, we specifically need to compute the minimum cost path between all the aforementioned products from the dot product calculation. In the next section, we will show how we can override the standard $+$ and $\times$ operations such that simply computing the squares of adjacency matrices allows us to solve \texttt{APSP}, and we will also explain how other properties of environments with overridden $+$ and $\times$ operations may help us solve \texttt{APSP} more efficiently.

\subsection{Abstract Algebra Overview and Ring Operations}

In this section, we cover the basics of the field of abstract algebra as relevant in solving the \texttt{APSP} problem. As we showed in section~\ref{sec:3.1}, on an adjacency matrix of power $A^{2 \times n}$, the $(s, t)$ entry encodes information regarding the dot product between $s$-$t$ paths involving up to $n$ intermediate nodes. When we explained our theory regarding Floyd-Warshall's algorithm, we mentioned that we somehow need to store the minimum cost path between all of the path costs ($s \to t, s \to k_0 \to t, \ldots, s \to k_0 \to k_1 \to \cdots \to k_{n - 1} \to t$) that make up the individual products in the dot product. We may now ask ourselves, when computing a dot product in the form:
\[
    \begin{split}
        A^2_{(s,t)}
        &= \mathrm{dist}_{s \to s} \times \mathrm{dist}_{s \to t} \\
        &+ \; \mathrm{dist}_{s \to k} \times \mathrm{dist}_{k \to t} \\
        &+ \; \mathrm{dist}_{s \to t} \times \mathrm{dist}_{t \to t}
    \end{split}
\]
Is it possible to override the $+$ and $\times$ operations in order to suit our desired $\min$ and $+$ operations?

It turns out this is entirely possible as long as our overridden $+$ and $\times$ operations adhere to the rules of \emph{algebraic rings}. A ring is an algebraic structure—that is, a set of elements together with two operations $\oplus$ and $\otimes$—that satisfies certain axioms:

\begin{quote}
    $S$ is an algebraic \emph{semiring} defined as $(S, \oplus, \otimes)$. The below conditions must be met:

    \begin{enumerate}
        \item $(S, \oplus)$ is an Abelian group. In other words, it satisfies the commutative property that $\forall a, b \in S : a \oplus b = b \oplus a$ and it satisfies the identity that $\forall a \in S : a \oplus 0_S = a$, where $0_S$ refers to the zero element in $S$.
        \item $(S, \otimes)$ is a monoid. In other words, it satisfies the identity $\forall a \in S : a \otimes I = a$, where $I$ refers to the identity element, which can typically be thought of as 1.
        \item The $\otimes$ operation must distribute over the $\oplus$ operation. In other words, $\forall a, b, c \in S : a \otimes (b \oplus c) = a \otimes b \oplus a \otimes c$.
        \item $0_S$ is an annihilator in $\otimes$. In other words, $\forall a \in S : a \otimes 0_S = 0_S \otimes a = 0_S$.
    \end{enumerate}

    {\footnotesize Note, the difference between a \emph{ring} and a \emph{semiring} is that a semiring does not need to support the additive inverse. Meaning, there does not necessarily have to be a ``negative'' corresponding to each element}
\end{quote}

So how does this tie into our \texttt{APSP} problem?

We can define our dot product over the semiring $(S, \oplus, \otimes)$ where:
\[
    \begin{split}
        & a, b \in \mathds{R} \\
        & a \oplus b = \min\{a, b\} \\
        & a \otimes b = a + b \qquad \text{{\footnotesize Where + is the typical addition}}
    \end{split}
\]
It is straightforward why we can do this, since $(S, \min, +)$ satisfies all of the axioms. You can convince yourself of this fact. It is important to note that $S$ does not have an additive inverse since there is no possible $a, b \in S : \min\{a, b\} = \infty$ unless $a = \infty$ and $b = \infty$.

Now, notice how the dot product defined at the end of section~\ref{sec:3.1} changes. Under our ring $S$, we get:
\[
    \begin{split}
        A^2_{(s,t)}
        &= \mathrm{dist}_{s \to s} \otimes \mathrm{dist}_{s \to t} \\
        &\oplus \; \mathrm{dist}_{s \to k} \otimes \mathrm{dist}_{k \to t} \\
        &\oplus \; \mathrm{dist}_{s \to t} \otimes \mathrm{dist}_{t \to t}
    \end{split}
    \qquad = \qquad
    \min\!\left\{
        \begin{aligned}
            &\mathrm{dist}_{s\to s} + \mathrm{dist}_{s\to t},\\
            &\mathrm{dist}_{s\to k} + \mathrm{dist}_{k\to t},\\
            &\mathrm{dist}_{s\to t} + \mathrm{dist}_{t\to t}
        \end{aligned}
    \right\}
\]
This exactly what we wanted to compute when we discussed Floyd Warshall's algorithm! As you can see, using algebraic rings, we have come up with a system where simple repeated squaring of the adjacency matrix $A$ over the ring $R$ effectively solves \texttt{APSP}. We notice that after squaring the adjacency matrix $n - 1$ times, the result contains the answer to \texttt{APSP}. Since matrix multiplication takes on the order of $\bigO(v^3)$ time, where $v$ is the number of vertices in the graph, and since we must effectively square the adjacency matrix $v$ times, we can see how a min-plus squaring algorithm attains a time complexity of $\bigO(v^4)$ (using naive matrix multiplication).

Our repeated squaring approach can take as little as $\bigO(n^3 \log(n))$ time if we are more efficient in how we implement \emph{binary exponentiation} rather than using naive matrix multiplication for exponentiation. However, the repeated squaring method is still more inefficient than Floyd-Warshall's algorithm, which takes only $\bigO(n^3)$ time. It turns out that both the min-plus repeated squaring approach and Floyd-Warshall's algorithm are performing the same operations to compute \texttt{APSP}, however Floyd-Warshall's algorithm attains more efficiency using \emph{dynamic programming} and ``simulating'' the min-plus matrix multiplications in place. This min-plus repeated squaring approach also lies at the heart of the algorithm mentioned in Jan van den Brand's paper, but before we can conceptualize it, we are going to use our knowledge or algebraic rings to talk about polynomial matrices.

\subsection{Polynomial Matrices}
\subsection{Edge Updates}
\subsection{Hitting Sets}

\section{Solution}
\subsection{Algorithm}
\subsection{Time Complexity Analysis}

\section{Extension and Comparison}
\subsection{Broader Algebraic Structures}
\subsection{General Update Query}
\subsection{Multiple Matrix Functions}
\subsection{Better Complexity Bounds}

\section*{References}

\begin{enumerate}
    \item \href{https://www.dropbox.com/scl/fi/zjfbk8dbxbzcn05dr8l11/fall22_daa_lecturenotes.pdf?rlkey=g6z7z1hvj7jknvlpy1s3dqcjw&e=1&st=13r18doq&dl=0}{Jan van den Brand's Notes}
    \item \href{https://www.dropbox.com/scl/fi/5w4k0lg0u8e36zugapp6c/Sankowski-COCOON-05-subquadratic-dynamic-distances.pdf?rlkey=5eahwsl7a49kwxyvlr7yqyoz3&e=2&st=d3zo7owc&dl=0}{Sankowski's Paper}
\end{enumerate}

\end{document}
